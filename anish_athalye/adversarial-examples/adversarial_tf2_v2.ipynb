{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Because we switch to `tf2`, instead of using `tf.slim`, we will just use `tf.keras` for the inception model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "tf.__version__, keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_model = keras.applications.inception_v3.InceptionV3()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "inception_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to find a way to make the weigths of `inception_model` constant, instead of `tf.Variable`, otherwise, we cannot train for an adversarial attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_dunder(obj, strict=False):\n",
    "    if strict:\n",
    "        prefix = \"_\"\n",
    "    else:\n",
    "        prefix = \"__\"\n",
    "    return [s for s in dir(obj) if not s.startswith(prefix)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "non_dunder(inception_model, True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "help(inception_model.stop_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_model.trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_model.trainable_weights is inception_model.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_model.trainable_weights == inception_model.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inception_model.trainable_weights, inception_model.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_model.trainable = False\n",
    "inception_model.trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inception_model.trainable_weights, inception_model.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inception_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_model.layers[0].input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.Variable(tf.zeros((299, 299, 3)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "adversarial_model = keras.Model(\n",
    "    inputs=image,\n",
    "    outputs=inception_model(image),\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ValueError: Input 0 is incompatible with layer inception_v3: expected shape=(None, 299, 299, 3), found shape=(299, 299, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.Variable(tf.zeros((1, 299, 299, 3)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "adversarial_model = keras.Model(\n",
    "    inputs=image,\n",
    "    outputs=inception_model(image),\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ValueError: Input tensors to a Functional must come from `tf.keras.Input`. Received: <tf.Variable 'Variable:0'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How should we solve this question?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that although we cannot make a model using `tf.Variable` as input,\n",
    "we can make `image` input of `inception_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.expand_dims(tf.zeros((299,299,3)), axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.expand_dims(np.zeros((299,299,3)), axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.expand_dims(tf.zeros((299,299,3)), axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.zeros((299,299,3)).ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros((299,299,3)).ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Wow, `tf` and `np` interplay harmoniously.\n",
    "\n",
    "It is inconvenient to have to remember to add an extra dimension to make the input to `inception_model` into a batch. Let's write a wrapper around it to make our life easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception(image):\n",
    "    if image.ndim == 3:\n",
    "        image = tf.expand_dims(image, axis=0)\n",
    "    return inception_model(image)[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "image = tf.Variable(tf.zeros((299, 299, 3)))\n",
    "inception(image)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "AttributeError: 'ResourceVariable' object has no attribute 'ndim'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "image = tf.Variable(tf.zeros((299, 299, 3)))\n",
    "image.ndim"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "AttributeError: 'ResourceVariable' object has no attribute 'ndim'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.Variable(tf.zeros((299, 299, 3)))\n",
    "len(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception(image):\n",
    "    if len(image.shape) == 3:\n",
    "        image = tf.expand_dims(image, axis=0)\n",
    "    return inception_model(image)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.Variable(tf.zeros((299, 299, 3)))\n",
    "inception(image).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's play with a real image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_json = Path(\"imagenet.json\")\n",
    "if not imagenet_json.exists():\n",
    "    imagenet_json, _ = urlretrieve(\n",
    "        'https://www.anishathalye.com/media/2017/07/25/imagenet.json'\n",
    "    )\n",
    "\n",
    "with open(imagenet_json) as f:\n",
    "    imagenet_labels = json.load(f)\n",
    "imagenet_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(imagenet_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\"desktop computer\"` sounds cool. Let's make it our target label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_path = Path(\"cat.jpg\")\n",
    "if not img_path.exists():\n",
    "    img_path, _ = urlretrieve('https://www.anishathalye.com/media/2017/07/25/cat.jpg')\n",
    "img_class = imagenet_labels.index(\"desktop computer\")\n",
    "imagenet_labels[img_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = PIL.Image.open(img_path)\n",
    "big_dim = max(img.width, img.height)\n",
    "wide = img.width > img.height\n",
    "new_w = 299 if not wide else int(img.width * 299 / img.height)\n",
    "new_h = 299 if wide else int(img.height * 299 / img.width)\n",
    "img = img.resize((new_w, new_h)).crop((0, 0, 299, 299))\n",
    "img = (np.asarray(img) / 255.0).astype(np.float32)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(?)** What is the `new_w, new_h, resize()` all about?<br>\n",
    "**(R)** The story was that Anish wanted to `crop((0,0,299,299))`, i.e. crop the `299x299` subimage for the upper left corner for each image. But the reality is that _**not every image has both width and height larger than**_ `299`. So the `resize((new_w, new_h))` was there to guarantee this. Indeed,\n",
    "\\begin{align}\n",
    "  h_{\\text{new}} = 299,\\;  w_{\\text{new}} = h_{\\text{new}} \\frac{w}{h} \\quad\\text{when}\\quad h < w \\\\\n",
    "  w_{\\text{new}} = 299,\\;  h_{\\text{new}} = w_{\\text{new}} \\frac{h}{w} \\quad\\text{when}\\quad h \\ge w\n",
    "\\end{align}\n",
    "\n",
    "which converted into words says **_always convert the shorter side to_** `299` and **_the longer side to its rightful length according to the original ratio_**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.rand(7)[[1,3,5]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "list(range(7))[[1,3,5]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TypeError: list indices must be integers or slices, not list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that `tf.Tensor`, like `list`, cannot access its subarray in this manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = inception_model(np.expand_dims(img, axis=0))[0]\n",
    "p.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "p[[1,3,4]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "InvalidArgumentError: Index out of range using input dim 1; input has only 1 dims [Op:StridedSlice] name: strided_slice/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.numpy()[[1,3,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(img, correct_class=None, target_class=None):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 8))\n",
    "    fig.sca(ax1)  # sca() Select Current Axis\n",
    "    #p = inception_model(np.expand_dims(img, axis=0))[0]\n",
    "    p = inception(img)\n",
    "    p = p.numpy()\n",
    "    print(f\"type(p) = {type(p)}, len(p) = {len(p)}\")\n",
    "    ax1.imshow(img)\n",
    "    #fig.sca(ax1)  # Why should there be two fig.sca(ax1)?\n",
    "    \n",
    "    # display the top 10 prediceted classes\n",
    "    #topk = list(p.argsort()[-10:][::-1])\n",
    "    #topk = list(tf.argsort(p)[-10:][::-1])\n",
    "    topk = list((tf.argsort(p)[-10:][::-1]).numpy())\n",
    "    print(f\"topk = {topk}\")\n",
    "    topprobs = p[topk]\n",
    "    barlist = ax2.bar(range(10), topprobs)\n",
    "    if target_class in topk:\n",
    "        barlist[topk.index(target_class)].set_color('r')\n",
    "    if correct_class in topk:\n",
    "        barlist[topk.index(correct_class)].set_color('g')\n",
    "    plt.sca(ax2)\n",
    "    plt.ylim([0, 1.1])\n",
    "    plt.xticks(range(10),\n",
    "               [imagenet_labels[i][:15] for i in topk],\n",
    "               rotation='vertical')\n",
    "    # :15 means \"taking the first 15 characters\" for fear of long-string class.\n",
    "    fig.subplots_adjust(bottom=0.2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "help(np.expand_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "DARK_READER = True\n",
    "if DARK_READER:\n",
    "    plt.rcParams.update({\n",
    "        \"lines.color\": \"white\",\n",
    "        \"patch.edgecolor\": \"white\",\n",
    "        \"text.color\": \"black\",\n",
    "        \"axes.facecolor\": \"black\",\n",
    "        \"axes.edgecolor\": \"lightgray\",\n",
    "        \"axes.labelcolor\": \"white\",\n",
    "        \"axes.titlecolor\": \"white\",\n",
    "        \"xtick.color\": \"white\",\n",
    "        \"ytick.color\": \"white\",\n",
    "        \"grid.color\": \"lightgray\",\n",
    "        \"figure.facecolor\": \"black\",\n",
    "        \"figure.edgecolor\": \"black\",\n",
    "        \"savefig.facecolor\": \"black\",\n",
    "        \"savefig.edgecolor\": \"black\",\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classify(img, correct_class=img_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial examples\n",
    "\n",
    "Given an image $\\mathbf{x}$, our neural network outputs a probability distribution over labels, $P(y \\mid \\mathbf{x})$. When we craft an adversarial input, we want to find an $\\hat{\\mathbf{x}}$ where $\\log P(\\hat{y} \\mid \\hat{\\mathbf{x}})$ is maximized for a target label $\\hat{y}$: that way, our input will be misclassified as the target class. We can ensure that $\\hat{\\mathbf{x}}$ doesn't look too different from the original $\\mathbf{x}$ by constraining ourselves to some $\\ell_\\infty$ <del>box</del> ball with radius $\\epsilon$, requiring that $\\left\\lVert \\mathbf{x} - \\hat{\\mathbf{x}} \\right\\rVert_\\infty \\le \\epsilon$.\n",
    "\n",
    "In this framework, an adversarial example is the solution to a constrained optimization problem that we can solve using [backpropagation](https://colah.github.io/posts/2015-08-Backprop/) and projected gradient descent, basically the same techniques that are used to train networks themselves. The algorithm is simple:\n",
    "\n",
    "We begin by initializing our adversarial example as $\\hat{\\mathbf{x}} \\leftarrow \\mathbf{x}$. Then, we repeat the following until convergence:\n",
    "\n",
    "1. $\\hat{\\mathbf{x}} \\leftarrow \\hat{\\mathbf{x}} + \\alpha \\cdot \\nabla \\log P(\\hat{y} \\mid \\hat{\\mathbf{x}})$\n",
    "2. $\\hat{\\mathbf{x}} \\leftarrow \\mathrm{clip}(\\hat{\\mathbf{x}}, \\mathbf{x} - \\epsilon, \\mathbf{x} + \\epsilon)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.zeros?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hat = tf.Variable(tf.zeros((229,229,3), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_epsilon = 2.0/255.0 # a really small perturbation\n",
    "demo_lr = 1e-1\n",
    "demo_steps = 100\n",
    "# Test other target classes at your own will\n",
    "#demo_target = imagenet_labels.index(\"pizza, pizza pie\")\n",
    "demo_target = imagenet_labels.index(\"desktop computer\")\n",
    "\n",
    "\n",
    "# initialization step\n",
    "#sess.run(assign_op, feed_dict={x: img})\n",
    "below = img - demo_epsilon\n",
    "above = img + demo_epsilon\n",
    "#x_hat = image\n",
    "\n",
    "# projected gradient descent\n",
    "for i in tqdm(range(demo_steps)):\n",
    "    ## gradient descent step\n",
    "    #_, loss_value = sess.run(\n",
    "    #    [optim_step, loss],\n",
    "    #    feed_dict={learning_rate: demo_lr, y_hat: demo_target})\n",
    "    ## project step\n",
    "    #sess.run(project_step, feed_dict={x: img, epsilon: demo_epsilon})\n",
    "    with tf.GradientTape() as tape:\n",
    "        log_likelihood = inception(image)[demo_target]\n",
    "    grad = tape.gradient(log_likelihood, x_hat)\n",
    "    tf.assign_add(x_hat, demo_lr*grad)\n",
    "    projected = tf.clip_by_value(tf.clip_by_value(x_hat, below, above), 0, 1)\n",
    "    tf.assign(x_hat, projected)\n",
    "    if (i+1) % 10 == 0:\n",
    "        print('step %d, loss=%g' % (i+1, loss_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow.contrib.slim.nets as nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we set up the input image. We use a `tf.Variable` instead of a `tf.placeholder` because we will need it to be trainable. We can still feed it when we want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.Variable(tf.zeros((299, 299, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the Inception v3 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception(image, reuse):\n",
    "    preprocessed = tf.multiply(tf.subtract(tf.expand_dims(image, 0), 0.5), 2.0)\n",
    "    arg_scope = nets.inception.inception_v3_arg_scope(weight_decay=0.0)\n",
    "    with slim.arg_scope(arg_scope):\n",
    "        logits, _ = nets.inception.inception_v3(\n",
    "            preprocessed, 1001, is_training=False, reuse=reuse)\n",
    "        logits = logits[:,1:] # ignore background class\n",
    "        probs = tf.nn.softmax(logits) # probabilities\n",
    "    return logits, probs\n",
    "\n",
    "logits, probs = inception(image, reuse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `preprocessed = tf.multiply(tf.subtract(tf.expand_dims(image, 0), 0.5), 2.0)` line, I guess, was just to make the output real numbers lying in $[-1, 1]$ instead of $[0, 1]$.\n",
    "\n",
    "Next, we load pre-trained weights. This Inception v3 has a top-5 accuracy of 93.9%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from urllib.request import urlretrieve\n",
    "#from urllib import urlretrieve\n",
    "import tarfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original notebook, the url of the following cell were `https` but I don't know why it does not work any more; instead, one has to replace `https` by `http`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = tempfile.mkdtemp()\n",
    "inception_tarball, _ = urlretrieve(\n",
    "    'http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz')\n",
    "tarfile.open(inception_tarball, 'r:gz').extractall(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(?1)**\n",
    "- `urlretrieve()`'s return value\n",
    "- `tarfile.open`'s return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls $data_dir"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ls ${data_dir}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ls: cannot access '$/tmp/tmpvzku6vpk': No such file or directory"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ls $(data_dir)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "/bin/bash: line 1: data_dir: command not found\n",
    "02_remarked.ipynb  adversarial.ipynb  README.md  trash.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restore_vars = [\n",
    "    var for var in tf.global_variables()\n",
    "    if var.name.startswith('InceptionV3/')\n",
    "]\n",
    "saver = tf.train.Saver(restore_vars)\n",
    "saver.restore(sess, os.path.join(data_dir, 'inception_v3.ckpt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we write some code to show an image, classify it, and show the classification result."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!conda install -n homl1e matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_json, _ = urlretrieve(\n",
    "    'https://www.anishathalye.com/media/2017/07/25/imagenet.json')\n",
    "with open(imagenet_json) as f:\n",
    "    imagenet_labels = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(imagenet_labels), len(imagenet_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(img, correct_class=None, target_class=None):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 8))\n",
    "    fig.sca(ax1)  # sca() Select Current Axis\n",
    "    p = sess.run(probs, feed_dict={image: img})[0]\n",
    "    ax1.imshow(img)\n",
    "    fig.sca(ax1)  # Why should there be two fig.sca(ax1)?\n",
    "    \n",
    "    # display the top 10 prediceted classes\n",
    "    topk = list(p.argsort()[-10:][::-1])\n",
    "    topprobs = p[topk]\n",
    "    barlist = ax2.bar(range(10), topprobs)\n",
    "    if target_class in topk:\n",
    "        barlist[topk.index(target_class)].set_color('r')\n",
    "    if correct_class in topk:\n",
    "        barlist[topk.index(correct_class)].set_color('g')\n",
    "    plt.sca(ax2)\n",
    "    plt.ylim([0, 1.1])\n",
    "    plt.xticks(range(10),\n",
    "               [imagenet_labels[i][:15] for i in topk],\n",
    "               rotation='vertical')\n",
    "    # :15 means \"taking the first 15 characters\" for fear of long-string class.\n",
    "    fig.subplots_adjust(bottom=0.2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = [\"a\", \"b\", \"c\"]\n",
    "L.index(\"c\"), L.index(\"b\"), #L.index(\"z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example image\n",
    "\n",
    "We load our example image and make sure it's classified correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path, _ = urlretrieve('https://www.anishathalye.com/media/2017/07/25/cat.jpg')\n",
    "img_class = 281\n",
    "img = PIL.Image.open(img_path)\n",
    "big_dim = max(img.width, img.height)\n",
    "wide = img.width > img.height\n",
    "new_w = 299 if not wide else int(img.width * 299 / img.height)\n",
    "new_h = 299 if wide else int(img.height * 299 / img.width)\n",
    "img = img.resize((new_w, new_h)).crop((0, 0, 299, 299))\n",
    "img = (np.asarray(img) / 255.0).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(?)** What is the `new_w, new_h, resize()` all about?<br>\n",
    "**(R)** The story was that Anish wanted to `crop((0,0,299,299))`, i.e. crop the `299x299` subimage for the upper left corner for each image. But the reality is that _**not every image has both width and height larger than**_ `299`. So the `resize((new_w, new_h))` was there to guarantee this. Indeed,\n",
    "\\begin{align}\n",
    "  h_{\\text{new}} = 299,\\;  w_{\\text{new}} = h_{\\text{new}} \\frac{w}{h} \\quad\\text{when}\\quad h < w \\\\\n",
    "  w_{\\text{new}} = 299,\\;  h_{\\text{new}} = w_{\\text{new}} \\frac{h}{w} \\quad\\text{when}\\quad h \\ge w\n",
    "\\end{align}\n",
    "\n",
    "which converted into words says **_always convert the shorter side to_** `299` and **_the longer side to its rightful length according to the original ratio_**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_labels[img_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classify(img, correct_class=img_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial examples\n",
    "\n",
    "Given an image $\\mathbf{x}$, our neural network outputs a probability distribution over labels, $P(y \\mid \\mathbf{x})$. When we craft an adversarial input, we want to find an $\\hat{\\mathbf{x}}$ where $\\log P(\\hat{y} \\mid \\hat{\\mathbf{x}})$ is maximized for a target label $\\hat{y}$: that way, our input will be misclassified as the target class. We can ensure that $\\hat{\\mathbf{x}}$ doesn't look too different from the original $\\mathbf{x}$ by constraining ourselves to some $\\ell_\\infty$ <del>box</del> ball with radius $\\epsilon$, requiring that $\\left\\lVert \\mathbf{x} - \\hat{\\mathbf{x}} \\right\\rVert_\\infty \\le \\epsilon$.\n",
    "\n",
    "In this framework, an adversarial example is the solution to a constrained optimization problem that we can solve using [backpropagation](https://colah.github.io/posts/2015-08-Backprop/) and projected gradient descent, basically the same techniques that are used to train networks themselves. The algorithm is simple:\n",
    "\n",
    "We begin by initializing our adversarial example as $\\hat{\\mathbf{x}} \\leftarrow \\mathbf{x}$. Then, we repeat the following until convergence:\n",
    "\n",
    "1. $\\hat{\\mathbf{x}} \\leftarrow \\hat{\\mathbf{x}} + \\alpha \\cdot \\nabla \\log P(\\hat{y} \\mid \\hat{\\mathbf{x}})$\n",
    "2. $\\hat{\\mathbf{x}} \\leftarrow \\mathrm{clip}(\\hat{\\mathbf{x}}, \\mathbf{x} - \\epsilon, \\mathbf{x} + \\epsilon)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "We start with the easiest part: writing a TensorFlow op for initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, (299, 299, 3))\n",
    "\n",
    "x_hat = image # our trainable adversarial input\n",
    "assign_op = tf.assign(x_hat, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent step\n",
    "\n",
    "Next, we write the gradient descent step to maximize the log probability of the target class (or equivalently, minimize the [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(())  # empty tuple as the shape of a float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = tf.placeholder(tf.float32, ())\n",
    "y_hat = tf.placeholder(tf.int32, ())\n",
    "\n",
    "labels = tf.one_hot(y_hat, 1000)\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=[labels])\n",
    "optim_step = tf.train.GradientDescentOptimizer(\n",
    "    learning_rate).minimize(loss, var_list=[x_hat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**N.B.** `logits` was defined in an earlier cell, via `image`, which should probably viewed as identical to `x_hat`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(?)** The `var_list` param of `minimize()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection step\n",
    "\n",
    "Finally, we write the projection step to keep our adversarial example visually close to the original image. Additionally, we clip to $[0, 1]$ to keep it a valid image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "epsilon = tf.placeholder(tf.float32, ())\n",
    "\n",
    "below = x - epsilon\n",
    "above = x + epsilon\n",
    "projected = tf.clip_by_value(tf.clip_by_value(x_hat, below, above), 0, 1)\n",
    "with tf.control_dependencies([projected]):\n",
    "    # cf. p.323 ageron's homl1e\n",
    "    project_step = tf.assign(x_hat, projected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(?)** What would happen if we do not use `tf.control_dependencies()` here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution\n",
    "\n",
    "Finally, we're ready to synthesize an adversarial example. We arbitrarily choose \"guacamole\" (imagenet class 924) as our target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_labels.index(\"pizza, pizza pie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_epsilon = 2.0/255.0 # a really small perturbation\n",
    "demo_lr = 1e-1\n",
    "demo_steps = 100\n",
    "demo_target = 924 # \"guacamole\"\n",
    "# Test other target classes at your own will\n",
    "demo_target = imagenet_labels.index(\"pizza, pizza pie\")\n",
    "\n",
    "\n",
    "# initialization step\n",
    "sess.run(assign_op, feed_dict={x: img})\n",
    "\n",
    "# projected gradient descent\n",
    "for i in range(demo_steps):\n",
    "    # gradient descent step\n",
    "    _, loss_value = sess.run(\n",
    "        [optim_step, loss],\n",
    "        feed_dict={learning_rate: demo_lr, y_hat: demo_target})\n",
    "    # project step\n",
    "    sess.run(project_step, feed_dict={x: img, epsilon: demo_epsilon})\n",
    "    if (i+1) % 10 == 0:\n",
    "        print('step %d, loss=%g' % (i+1, loss_value))\n",
    "    \n",
    "\n",
    "adv = x_hat.eval() # retrieve the adversarial example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rmk.**\n",
    "- Took several minutes to finish running `100` steps on Thinkpad X200.\n",
    "- [%g](https://stackoverflow.com/questions/30658919/the-precision-of-printf-with-specifier-g)\n",
    "- Note that `image` and `x_hat` really are regarded as identical in the TensorFlow graph above, because\n",
    "  - `project_step` assigns the projection to `x_hat`\n",
    "  - Optimization updates `image`\n",
    "  - They have to be the same thing in order for this to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This adversarial image is visually indistinguishable from the original, with no visual artifacts. However, it's classified as \"guacamole\" with high probability!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(adv, correct_class=img_class, target_class=demo_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(?)** How small was the perturbation `2.0/255.0`? Try to do an experiment with random noise of that magnitude of perturbation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stopped here (2021/02/19 (金) 16h04)**\n",
    "\n",
    "This adv attack is very cool, but we haven't verified that the weights we downloaded really classify the imagenet images well. Let's do this."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import tensorflow_datasets as tfds\n",
    "ds = tfds.load(\"imagenet2012_subset/1pct\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------\n",
    "ModuleNotFoundError                       Traceback (most recent call last)\n",
    "<ipython-input-32-188e57836169> in <module>\n",
    "----> 1 import tensorflow_datasets as tfds\n",
    "      2 ds = tfds.load(\"imagenet2012_subset/1pct\")\n",
    "      3 ds\n",
    "\n",
    "ModuleNotFoundError: No module named 'tensorflow_datasets'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!conda install -n homl1e tensorflow-datasets -y"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!which pip"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip uninstall -y tensorflow_datasets"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import tensorflow_datasets as tfds\n",
    "ds, ds_info = tfds.load(\"imagenet2012_subset/1pct\", with_info=True)\n",
    "ds_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that one cannot use `tfds` in `tensorflow==1.13.1`.<br>\n",
    "`pip` seems to download ok, but cannot run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust adversarial examples\n",
    "\n",
    "Now, we go through a more advanced example. We follow our approach for [synthesizing robust adversarial examples](https://arxiv.org/abs/1707.07397) to find a single perturbation of our cat image that's simultaneously adversarial under some chosen distribution of transformations.  We could choose any distribution of differentiable transformations; in this post, we'll synthesize a single adversarial input that's robust to rotation by $\\theta \\in [-\\pi/4, \\pi/4]$.\n",
    "\n",
    "Before we proceed, let's check if our previous example is still adversarial if we rotate it, say by an angle of $\\theta = \\pi/8$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_angle = np.pi/8\n",
    "\n",
    "angle = tf.placeholder(tf.float32, ())\n",
    "rotated_image = tf.contrib.image.rotate(image, angle)\n",
    "rotated_example = rotated_image.eval(feed_dict={image: adv, angle: ex_angle})\n",
    "classify(rotated_example, correct_class=img_class, target_class=demo_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like our original adversarial example is not rotation-invariant! Let's try one more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotated_example = rotated_image.eval(feed_dict={image: adv, angle: -np.pi/4})\n",
    "classify(rotated_example, correct_class=img_class, target_class=demo_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how do we make an adversarial example robust to a distribution of transformations? Given some distribution of transformations $T$, we can maximize $\\mathbb{E}_{t \\sim T} \\log P\\left(\\hat{y} \\mid t(\\hat{\\mathbf{x}})\\right)$, subject to $\\left\\lVert \\mathbf{x} - \\hat{\\mathbf{x}} \\right\\rVert_\\infty \\le \\epsilon$. We can solve this optimization problem via projected gradient descent, noting that $\\nabla \\mathbb{E}_{t \\sim T} \\log P\\left(\\hat{y} \\mid t(\\hat{\\mathbf{x}})\\right)$ is $\\mathbb{E}_{t \\sim T} \\nabla \\log P\\left(\\hat{y} \\mid t(\\hat{\\mathbf{x}})\\right)$ and approximating with samples at each gradient descent step.\n",
    "\n",
    "Rather than manually implementing the gradient sampling, we can use a trick to get TensorFlow to do it for us: we can model our sampling-based gradient descent as doing gradient descent over an ensemble of stochastic classifiers that randomly sample from the distribution and transform their input before classifying it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(?)** Why does Anish keep speak of **distribution of transformations**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "num_samples = 10\n",
    "average_loss = 0\n",
    "for i in range(num_samples):\n",
    "    rotated = tf.contrib.image.rotate(\n",
    "        image, tf.random_uniform((), minval=-np.pi/4, maxval=np.pi/4))\n",
    "    rotated_logits, _ = inception(rotated, reuse=True)\n",
    "    average_loss += tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=rotated_logits, labels=labels) / num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(?)** What is `reuse=True` here? It means that we reuse the weights we've found before?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(?)** Why `average_loss` has to be divided by `num_samples` here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(?)** Why in the last `tf.nn.softmax_cross_entropy_with_logits()` we had `labels=[labels]` while in this one we have `labels=labels`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reuse our `assign_op` and `project_step`, though we'll have to write a new `optim_step` for this new objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_step = tf.train.GradientDescentOptimizer(\n",
    "    learning_rate).minimize(average_loss, var_list=[x_hat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we're ready to run PGD to generate our adversarial input. As in the previous example, we'll choose \"guacamole\" as our target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "demo_epsilon = 8.0/255.0 # still a pretty small perturbation\n",
    "demo_lr = 2e-1\n",
    "demo_steps = 300\n",
    "demo_target = 924 # \"guacamole\"\n",
    "\n",
    "# initialization step\n",
    "sess.run(assign_op, feed_dict={x: img})\n",
    "\n",
    "# projected gradient descent\n",
    "for i in range(demo_steps):\n",
    "    # gradient descent step\n",
    "    _, loss_value = sess.run(\n",
    "        [optim_step, average_loss],\n",
    "        feed_dict={learning_rate: demo_lr, y_hat: demo_target})\n",
    "    # project step\n",
    "    sess.run(project_step, feed_dict={x: img, epsilon: demo_epsilon})\n",
    "    if (i+1) % 50 == 0:\n",
    "        print('step %d, loss=%g' % (i+1, loss_value))\n",
    "    \n",
    "\n",
    "adv_robust = x_hat.eval() # retrieve the adversarial example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This adversarial image is classified as \"guacamole\" with high confidence, even when it's rotated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotated_example = rotated_image.eval(feed_dict={image: adv_robust, angle: ex_angle})\n",
    "#rotated_example = rotated_image.eval(feed_dict={image: adv_robust, angle: np.random.uniform(-np.pi/4, np.pi/4)})\n",
    "classify(rotated_example, correct_class=img_class, target_class=demo_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Let's examine the rotation-invariance of the robust adversarial example we produced over the entire range of angles, looking at $P(\\hat{y} \\mid \\hat{\\mathbf{x}})$ over $\\theta \\in [-\\pi/4, \\pi/4]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = np.linspace(-np.pi/4, np.pi/4, 301)\n",
    "\n",
    "p_naive = []\n",
    "p_robust = []\n",
    "for theta in thetas:\n",
    "    rotated = rotated_image.eval(feed_dict={image: adv_robust, angle: theta})\n",
    "    p_robust.append(probs.eval(feed_dict={image: rotated})[0][demo_target])\n",
    "    \n",
    "    rotated = rotated_image.eval(feed_dict={image: adv, angle: theta})\n",
    "    p_naive.append(probs.eval(feed_dict={image: rotated})[0][demo_target])\n",
    "\n",
    "robust_line, = plt.plot(thetas, p_robust, color='b', linewidth=2, label='robust')\n",
    "naive_line, = plt.plot(thetas, p_naive, color='r', linewidth=2, label='naive')\n",
    "plt.ylim([0, 1.05])\n",
    "plt.xlabel('rotation angle')\n",
    "plt.ylabel('target class probability')\n",
    "plt.legend(handles=[robust_line, naive_line], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's super effective!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "It would be a good challenge to write this same notebook using\n",
    "- `tf2`\n",
    "- `torch`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
